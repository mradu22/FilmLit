{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils, utils\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname='TEST_LSI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mtok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f9cf96a65bc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;31m#if __name__ == '__main__':\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[0mis_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-f9cf96a65bc2>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(is_train)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextSimilar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_pre\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[0mts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-f9cf96a65bc2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, fname, is_pre, method, **params)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_conf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_pre\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m#self.docs = cPickle.load(open(self.conf['fname_docs']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-f9cf96a65bc2>\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fname_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mtok' is not defined"
     ]
    }
   ],
   "source": [
    "# convert to unicode\n",
    "\n",
    "\n",
    "\n",
    "def to_unicode(text):\n",
    "    if not isinstance(text, unicode):\n",
    "        text = text.decode('utf-8')\n",
    "    return text\n",
    "\n",
    "class TextSimilar(utils.SaveLoad):\n",
    "    def __init__(self):\n",
    "        self.conf = {}\n",
    "\n",
    "    def _preprocess(self):\n",
    "        subd=pd.read_hdf('Sub_data_cleaned.h5', 'subs')\n",
    "        movd=pd.read_hdf('movbaseB.h5', 'imdb_id')\n",
    "        \n",
    "        raw={}  #dictionary with imdb_id as keys and string of entire script (uncleaned)\n",
    "        mtok={} #dictionary with imdb_id as keys and tokenized list of words for script\n",
    "        mltk={} #dictionary with imdb_id as keys and nltk.text.Text\n",
    "        fdist={} #dictionary with imdb_id as keys and nltk.probability.FreqDist\n",
    "\n",
    "        for item in subd.columns.levels[0]:\n",
    "                raw[item]=\" \".join(subd[item]['Line']).strip().lower()\n",
    "                raw[item]=re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', raw[item]) #removes web addressses\n",
    "                raw[item]=re.sub('<[^>]+>', '', raw[item]) #removes < crap > \n",
    "                raw[item]=re.sub('xe2\\w+', '', raw[item]) #removes weird shit like xe2x99xaa\n",
    "                raw[item]=re.sub('xc2\\w+', '', raw[item]) #removes weird shit like xe2x99xaa\n",
    "                raw[item]=re.sub('x9.*? ', '', raw[item]) #removes weird shit like x92 \n",
    "                raw[item]=re.sub('xa7.*? ', '', raw[item])\n",
    "                mtok[item]=[w for w in tokenizer.tokenize(raw[item]) if not w in stop_words]\n",
    "        \n",
    "        \n",
    "        docs = list(mtok.values())\n",
    "        dictionary = corpora.Dictionary(docs)\n",
    "        dictionary.save(self.conf['fname_dict'])\n",
    "\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "        corpora.MmCorpus.serialize(self.conf['fname_corpus'], corpus)\n",
    "\n",
    "        return docs, dictionary, corpus\n",
    "\n",
    "    def _generate_conf(self):\n",
    "        fname = self.fname[self.fname.rfind('/') + 1:]\n",
    "        self.conf['fname_docs']   = '%s.docs' % fname\n",
    "        self.conf['fname_dict']   = '%s.dict' % fname\n",
    "        self.conf['fname_corpus'] = '%s.mm' % fname\n",
    "\n",
    "    def train(self, fname, is_pre=True, method='lsi', **params):\n",
    "        self.fname = fname\n",
    "        self.method = method\n",
    "        self._generate_conf()\n",
    "        if is_pre:\n",
    "            self.docs, self.dictionary, corpus = self._preprocess()\n",
    "        else:\n",
    "            #self.docs = cPickle.load(open(self.conf['fname_docs']))\n",
    "            docs = list(mtok.values())\n",
    "            self.dictionary = corpora.Dictionary.load(self.conf['fname_dict'])\n",
    "            corpus = corpora.MmCorpus(self.conf['fname_corpus'])\n",
    "\n",
    "        if params is None:\n",
    "            params = {}\n",
    "\n",
    "        logger.info(\"training TF-IDF model\")\n",
    "        self.tfidf = models.TfidfModel(corpus, id2word=self.dictionary)\n",
    "        corpus_tfidf = self.tfidf[corpus]\n",
    "\n",
    "        if method == 'lsi':\n",
    "            logger.info(\"training LSI model\")\n",
    "            self.lsi = models.LsiModel(corpus_tfidf, id2word=self.dictionary, **params)\n",
    "            self.similar_index = similarities.MatrixSimilarity(self.lsi[corpus_tfidf])\n",
    "            self.para = self.lsi[corpus_tfidf]\n",
    "        elif method == 'lda_tfidf':\n",
    "            logger.info(\"training LDA model\")\n",
    "            self.lda = models.LdaMulticore(corpus_tfidf, id2word=self.dictionary, workers=8, **params)\n",
    "            self.similar_index = similarities.MatrixSimilarity(self.lda[corpus_tfidf])\n",
    "            self.para = self.lda[corpus_tfidf]\n",
    "        elif method == 'lda':\n",
    "            logger.info(\"training LDA model\")\n",
    "            self.lda = models.LdaMulticore(corpus, id2word=self.dictionary, workers=8, **params)\n",
    "            self.similar_index = similarities.MatrixSimilarity(self.lda[corpus])\n",
    "            self.para = self.lda[corpus]\n",
    "        elif method == 'logentropy':\n",
    "            logger.info(\"training a log-entropy model\")\n",
    "            self.logent = models.LogEntropyModel(corpus, id2word=self.dictionary)\n",
    "            self.similar_index = similarities.MatrixSimilarity(self.logent[corpus])\n",
    "            self.para = self.logent[corpus]\n",
    "        else:\n",
    "            msg = \"unknown semantic method %s\" % method\n",
    "            logger.error(msg)\n",
    "            raise NotImplementedError(msg)\n",
    "\n",
    "    def doc2vec(self, doc):\n",
    "        bow = self.dictionary.doc2bow(to_unicode(doc).split())\n",
    "        if self.method == 'lsi':\n",
    "            return self.lsi[self.tfidf[bow]]\n",
    "        elif self.method == 'lda':\n",
    "            return self.lda[bow]\n",
    "        elif self.method == 'lda_tfidf':\n",
    "            return self.lda[self.tfidf[bow]]\n",
    "        elif self.method == 'logentropy':\n",
    "            return self.logent[bow]\n",
    "\n",
    "    def find_similar(self, doc, n=10):\n",
    "        vec = self.doc2vec(doc)\n",
    "        sims = self.similar_index[vec]\n",
    "        sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "        for elem in sims[:n]:\n",
    "            idx, value = elem\n",
    "            print(' '.join(self.docs[idx]), value)\n",
    "\n",
    "    def get_vectors(self):\n",
    "        return self._get_vector(self.para)\n",
    "\n",
    "    def _get_vector(self, corpus):\n",
    "\n",
    "        def get_max_id():\n",
    "            maxid = -1\n",
    "            for document in corpus:\n",
    "                maxid = max(maxid, max([-1] + [fieldid for fieldid, _ in document])) # [-1] to avoid exceptions from max(empty)\n",
    "            return maxid\n",
    "\n",
    "        num_features = 1 + get_max_id()\n",
    "        index = np.empty(shape=(len(corpus), num_features), dtype=np.float32)\n",
    "        for docno, vector in enumerate(corpus):\n",
    "            if docno % 1000 == 0:\n",
    "                print(\"PROGRESS: at document #%i/%i\" % (docno, len(corpus)))\n",
    "\n",
    "            if isinstance(vector, np.ndarray):\n",
    "                pass\n",
    "            elif scipy.sparse.issparse(vector):\n",
    "                vector = vector.toarray().flatten()\n",
    "            else:\n",
    "                vector = matutils.unitvec(matutils.sparse2full(vector, num_features))\n",
    "            index[docno] = vector        \n",
    "\n",
    "        return index\n",
    "\n",
    "\n",
    "def cluster(vectors, ts, k=30):\n",
    "    from sklearn.cluster import k_means\n",
    "    X = np.array(vectors)\n",
    "    cluster_center, result, inertia = k_means(X.astype(np.float), n_clusters=k, init=\"k-means++\")\n",
    "    X_Y_dic = defaultdict(set)\n",
    "    for i, pred_y in enumerate(result):\n",
    "        X_Y_dic[pred_y].add(''.join(ts.docs[i]))\n",
    "\n",
    "    print('len(X_Y_dic): ', len(X_Y_dic))\n",
    "    with open(data_dir + '/cluser.txt', 'w') as fo:\n",
    "        for Y in X_Y_dic:\n",
    "            fo.write(str(Y) + '\\n')\n",
    "            fo.write('{word}\\n'.format(word='\\n'.join(list(X_Y_dic[Y])[:100])))\n",
    "\n",
    "def main(is_train=True):\n",
    "    fname = data_dir + '/brand'\n",
    "\n",
    "    num_topics = 100\n",
    "    method = 'lda'\n",
    "\n",
    "    ts = TextSimilar()\n",
    "    if is_train:\n",
    "        ts.train(fname, method=method ,num_topics=num_topics, is_pre=True, iterations=100)\n",
    "        ts.save(method)\n",
    "    else:   \n",
    "        ts = TextSimilar().load(method)\n",
    "\n",
    "    index = ts.get_vectors()\n",
    "    cluster(index, ts, k=num_topics)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "is_train = True if len(sys.argv) > 1 else False\n",
    "main(is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
