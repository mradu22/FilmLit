{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils, utils\n",
    "import logging\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.cluster import k_means\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname='TEST_LSI'\n",
    "logger = logging.getLogger('text_similar')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('brand.dict','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.TextIOWrapper"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-10 13:37:27,772 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-11-10 13:37:30,239 : INFO : built Dictionary(76163 unique tokens: ['slave', 'magic', 'mirror', 'come', 'farthest']...) from 832 documents (total 3342957 corpus positions)\n",
      "2017-11-10 13:37:30,240 : INFO : saving Dictionary object under brand.dict, separately None\n",
      "2017-11-10 13:37:30,278 : INFO : saved brand.dict\n",
      "2017-11-10 13:37:32,206 : INFO : storing corpus in Matrix Market format to brand.mm\n",
      "2017-11-10 13:37:32,209 : INFO : saving sparse matrix to brand.mm\n",
      "2017-11-10 13:37:32,210 : INFO : PROGRESS: saving document #0\n",
      "2017-11-10 13:37:34,079 : INFO : saved 832x76163 matrix, density=1.736% (1099922/63367616)\n",
      "2017-11-10 13:37:34,087 : INFO : saving MmCorpus index to brand.mm.index\n",
      "2017-11-10 13:37:34,322 : INFO : training TF-IDF model\n",
      "2017-11-10 13:37:34,323 : INFO : collecting document frequencies\n",
      "2017-11-10 13:37:34,324 : INFO : PROGRESS: processing document #0\n",
      "2017-11-10 13:37:34,646 : INFO : calculating IDF weights for 832 documents and 76162 features (1099922 matrix non-zeros)\n",
      "2017-11-10 13:37:34,692 : INFO : training LDA model\n",
      "2017-11-10 13:37:34,696 : INFO : using symmetric alpha at 0.01\n",
      "2017-11-10 13:37:34,697 : INFO : using symmetric eta at 1.3129734910652154e-05\n",
      "2017-11-10 13:37:34,715 : INFO : using serial LDA version on this node\n",
      "2017-11-10 13:38:22,626 : INFO : running online LDA training, 100 topics, 1 passes over the supplied corpus of 832 documents, updating every 16000 documents, evaluating every ~832 documents, iterating 100x with a convergence threshold of 0.001000\n",
      "2017-11-10 13:38:22,627 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2017-11-10 13:38:22,652 : INFO : training LDA model using 8 processes\n",
      "2017-11-10 13:38:22,685 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #832/832, outstanding queue size 1\n",
      "2017-11-10 13:38:53,830 : INFO : topic #60 (0.010): 0.018*\"oh\" + 0.009*\"get\" + 0.009*\"right\" + 0.009*\"like\" + 0.008*\"got\" + 0.007*\"know\" + 0.007*\"yeah\" + 0.007*\"hey\" + 0.006*\"come\" + 0.006*\"good\"\n",
      "2017-11-10 13:38:53,833 : INFO : topic #89 (0.010): 0.017*\"go\" + 0.015*\"know\" + 0.012*\"yeah\" + 0.012*\"right\" + 0.010*\"come\" + 0.010*\"get\" + 0.009*\"oh\" + 0.007*\"one\" + 0.007*\"good\" + 0.007*\"like\"\n",
      "2017-11-10 13:38:53,836 : INFO : topic #21 (0.010): 0.013*\"know\" + 0.012*\"oh\" + 0.011*\"l\" + 0.011*\"right\" + 0.010*\"go\" + 0.009*\"well\" + 0.008*\"get\" + 0.008*\"one\" + 0.008*\"like\" + 0.008*\"yeah\"\n",
      "2017-11-10 13:38:53,839 : INFO : topic #92 (0.010): 0.012*\"one\" + 0.010*\"go\" + 0.010*\"right\" + 0.009*\"oh\" + 0.009*\"got\" + 0.009*\"know\" + 0.008*\"come\" + 0.008*\"get\" + 0.007*\"good\" + 0.007*\"going\"\n",
      "2017-11-10 13:38:53,841 : INFO : topic #97 (0.010): 0.009*\"like\" + 0.009*\"know\" + 0.009*\"oh\" + 0.008*\"right\" + 0.008*\"yeah\" + 0.008*\"get\" + 0.007*\"one\" + 0.006*\"got\" + 0.006*\"go\" + 0.006*\"well\"\n",
      "2017-11-10 13:38:53,884 : INFO : topic diff=47.646954, rho=1.000000\n",
      "2017-11-10 13:40:25,026 : INFO : -9.874 per-word bound, 938.1 perplexity estimate based on a held-out corpus of 832 documents with 3342957 words\n",
      "2017-11-10 13:40:25,153 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2017-11-10 13:40:40,293 : INFO : creating matrix with 832 documents and 99 features\n",
      "2017-11-10 13:40:55,343 : INFO : saving TextSimilar object under lda, separately None\n",
      "2017-11-10 13:40:58,437 : INFO : saved lda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: at document #0/832\n",
      "len(X_Y_dic):  100\n"
     ]
    }
   ],
   "source": [
    "# convert to unicode\n",
    "\n",
    "\n",
    "\n",
    "def to_unicode(text):\n",
    "    if not isinstance(text, unicode):\n",
    "        text = text.decode('utf-8')\n",
    "    return text\n",
    "\n",
    "class TextSimilar(utils.SaveLoad):\n",
    "    def __init__(self):\n",
    "        self.conf = {}\n",
    "\n",
    "    def _preprocess(self):\n",
    "        tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "        stop_words=set(stopwords.words('english'))\n",
    "        \n",
    "        subd=pd.read_hdf('Sub_data_cleaned.h5', 'subs')\n",
    "        movd=pd.read_hdf('movbaseB.h5', 'imdb_id')\n",
    "        \n",
    "        raw={}  #dictionary with imdb_id as keys and string of entire script (uncleaned)\n",
    "        mtok={} #dictionary with imdb_id as keys and tokenized list of words for script\n",
    "        mltk={} #dictionary with imdb_id as keys and nltk.text.Text\n",
    "        fdist={} #dictionary with imdb_id as keys and nltk.probability.FreqDist\n",
    "\n",
    "        for item in subd.columns.levels[0]:\n",
    "                raw[item]=\" \".join(subd[item]['Line']).strip().lower()\n",
    "                raw[item]=re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', raw[item]) #removes web addressses\n",
    "                raw[item]=re.sub('<[^>]+>', '', raw[item]) #removes < crap > \n",
    "                raw[item]=re.sub('xe2\\w+', '', raw[item]) #removes weird shit like xe2x99xaa\n",
    "                raw[item]=re.sub('xc2\\w+', '', raw[item]) #removes weird shit like xe2x99xaa\n",
    "                raw[item]=re.sub('x9.*? ', '', raw[item]) #removes weird shit like x92 \n",
    "                raw[item]=re.sub('xa7.*? ', '', raw[item])\n",
    "                mtok[item]=[w for w in tokenizer.tokenize(raw[item]) if not w in stop_words]\n",
    "        \n",
    "        \n",
    "        docs = list(mtok.values())\n",
    "        dictionary = corpora.Dictionary(docs)\n",
    "        dictionary.save(self.conf['fname_dict'])\n",
    "\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "        corpora.MmCorpus.serialize(self.conf['fname_corpus'], corpus)\n",
    "\n",
    "        return docs, dictionary, corpus\n",
    "\n",
    "    def _generate_conf(self):\n",
    "        fname = self.fname[self.fname.rfind('/') + 1:]\n",
    "        self.conf['fname_docs']   = '%s.docs' % fname\n",
    "        self.conf['fname_dict']   = '%s.dict' % fname\n",
    "        self.conf['fname_corpus'] = '%s.mm' % fname\n",
    "\n",
    "    def train(self, fname, is_pre=True, method='lsi', **params):\n",
    "        self.fname = fname\n",
    "        self.method = method\n",
    "        self._generate_conf()\n",
    "        if is_pre:\n",
    "            self.docs, self.dictionary, corpus = self._preprocess()\n",
    "        else:\n",
    "            #self.docs = cPickle.load(open(self.conf['fname_docs']))\n",
    "            docs = list(mtok.values())\n",
    "            self.dictionary = corpora.Dictionary.load(self.conf['fname_dict'])\n",
    "            corpus = corpora.MmCorpus(self.conf['fname_corpus'])\n",
    "\n",
    "        if params is None:\n",
    "            params = {}\n",
    "\n",
    "        logger.info(\"training TF-IDF model\")\n",
    "        self.tfidf = models.TfidfModel(corpus, id2word=self.dictionary)\n",
    "        corpus_tfidf = self.tfidf[corpus]\n",
    "\n",
    "        if method == 'lsi':\n",
    "            logger.info(\"training LSI model\")\n",
    "            self.lsi = models.LsiModel(corpus_tfidf, id2word=self.dictionary, **params)\n",
    "            self.similar_index = similarities.MatrixSimilarity(self.lsi[corpus_tfidf])\n",
    "            self.para = self.lsi[corpus_tfidf]\n",
    "        elif method == 'lda_tfidf':\n",
    "            logger.info(\"training LDA model\")\n",
    "            self.lda = models.LdaMulticore(corpus_tfidf, id2word=self.dictionary, workers=8, **params)\n",
    "            self.similar_index = similarities.MatrixSimilarity(self.lda[corpus_tfidf])\n",
    "            self.para = self.lda[corpus_tfidf]\n",
    "        elif method == 'lda':\n",
    "            logger.info(\"training LDA model\")\n",
    "            self.lda = models.LdaMulticore(corpus, id2word=self.dictionary, workers=8, **params)\n",
    "            self.similar_index = similarities.MatrixSimilarity(self.lda[corpus])\n",
    "            self.para = self.lda[corpus]\n",
    "        elif method == 'logentropy':\n",
    "            logger.info(\"training a log-entropy model\")\n",
    "            self.logent = models.LogEntropyModel(corpus, id2word=self.dictionary)\n",
    "            self.similar_index = similarities.MatrixSimilarity(self.logent[corpus])\n",
    "            self.para = self.logent[corpus]\n",
    "        else:\n",
    "            msg = \"unknown semantic method %s\" % method\n",
    "            logger.error(msg)\n",
    "            raise NotImplementedError(msg)\n",
    "\n",
    "    def doc2vec(self, doc):\n",
    "        bow = self.dictionary.doc2bow(to_unicode(doc).split())\n",
    "        if self.method == 'lsi':\n",
    "            return self.lsi[self.tfidf[bow]]\n",
    "        elif self.method == 'lda':\n",
    "            return self.lda[bow]\n",
    "        elif self.method == 'lda_tfidf':\n",
    "            return self.lda[self.tfidf[bow]]\n",
    "        elif self.method == 'logentropy':\n",
    "            return self.logent[bow]\n",
    "\n",
    "    def find_similar(self, doc, n=10):\n",
    "        vec = self.doc2vec(doc)\n",
    "        sims = self.similar_index[vec]\n",
    "        sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "        for elem in sims[:n]:\n",
    "            idx, value = elem\n",
    "            print(' '.join(self.docs[idx]), value)\n",
    "\n",
    "    def get_vectors(self):\n",
    "        return self._get_vector(self.para)\n",
    "\n",
    "    def _get_vector(self, corpus):\n",
    "\n",
    "        def get_max_id():\n",
    "            maxid = -1\n",
    "            for document in corpus:\n",
    "                maxid = max(maxid, max([-1] + [fieldid for fieldid, _ in document])) # [-1] to avoid exceptions from max(empty)\n",
    "            return maxid\n",
    "\n",
    "        num_features = 1 + get_max_id()\n",
    "        index = np.empty(shape=(len(corpus), num_features), dtype=np.float32)\n",
    "        for docno, vector in enumerate(corpus):\n",
    "            if docno % 1000 == 0:\n",
    "                print(\"PROGRESS: at document #%i/%i\" % (docno, len(corpus)))\n",
    "\n",
    "            if isinstance(vector, np.ndarray):\n",
    "                pass\n",
    "            elif scipy.sparse.issparse(vector):\n",
    "                vector = vector.toarray().flatten()\n",
    "            else:\n",
    "                vector = matutils.unitvec(matutils.sparse2full(vector, num_features))\n",
    "            index[docno] = vector        \n",
    "\n",
    "        return index\n",
    "\n",
    "\n",
    "def cluster(vectors, ts, k=30):\n",
    "    from sklearn.cluster import k_means\n",
    "    X = np.array(vectors)\n",
    "    cluster_center, result, inertia = k_means(X.astype(np.float), n_clusters=k, init=\"k-means++\")\n",
    "    X_Y_dic = defaultdict(set)\n",
    "    for i, pred_y in enumerate(result):\n",
    "        X_Y_dic[pred_y].add(''.join(ts.docs[i]))\n",
    "\n",
    "    print('len(X_Y_dic): ', len(X_Y_dic))\n",
    "    with open(data_dir + '/cluser.txt', 'w') as fo:\n",
    "        for Y in X_Y_dic:\n",
    "            fo.write(str(Y) + '\\n')\n",
    "            fo.write('{word}\\n'.format(word='\\n'.join(list(X_Y_dic[Y])[:100])))\n",
    "\n",
    "def main(is_train=True):\n",
    "    fname = data_dir + '/brand'\n",
    "\n",
    "    num_topics = 100\n",
    "    method = 'lda'\n",
    "\n",
    "    ts = TextSimilar()\n",
    "    if is_train:\n",
    "        ts.train(fname, method=method ,num_topics=num_topics, is_pre=True, iterations=100)\n",
    "        ts.save(method)\n",
    "    else:   \n",
    "        ts = TextSimilar().load(method)\n",
    "\n",
    "    index = ts.get_vectors()\n",
    "    cluster(index, ts, k=num_topics)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "is_train = True if len(sys.argv) > 1 else False\n",
    "main(is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_similar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b6a1fa9b1286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfind_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'find_similar' is not defined"
     ]
    }
   ],
   "source": [
    "find_similar(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'doc2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e94aa274ef86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTextSimilar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_similar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Hey there fucker. Dumbledore.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'doc2vec'"
     ]
    }
   ],
   "source": [
    "TextSimilar.find_similar.doc2vec('Hey there fucker. Dumbledore.',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
