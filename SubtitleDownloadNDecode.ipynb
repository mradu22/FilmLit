{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io, gzip\n",
    "from codecs import decode\n",
    "import pysrt\n",
    "import datetime\n",
    "import base64\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "import pdb\n",
    "import dill\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "from xmlrpc.client import ServerProxy\n",
    "import os as os\n",
    "import os.path\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('credentials.pkd','rb') as fp:\n",
    "    credentials = dill.load(fp)\n",
    "USER_AGENT=credentials['user']\n",
    "USERNAME=credentials['user']\n",
    "PASS=credentials['passw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in. Acquired token = Ioi9AY-QcMZr8agB5dFxai50yB2\n"
     ]
    }
   ],
   "source": [
    "OPENSUBTITLES_SERVER = 'http://api.opensubtitles.org/xml-rpc'\n",
    "LANGUAGE = 'en'\n",
    "\n",
    "xmlrpc = ServerProxy(OPENSUBTITLES_SERVER,allow_none=True)\n",
    "data = xmlrpc.LogIn(USERNAME, PASS, LANGUAGE, USER_AGENT)\n",
    "status=data.get('status')\n",
    "if status != '200 OK':\n",
    "    print('Could not log in. Something is wrong... ')\n",
    "    print(\"data.get('status')\")\n",
    "    token = None\n",
    "else:\n",
    "    token = data.get('token')\n",
    "    print('Logged in. Acquired token = {}'.format(token))\n",
    "def server_stat():\n",
    "    print('------------------Current Stats-----------------')\n",
    "    for key,item in xmlrpc.ServerInfo(token)['download_limits'].items():\n",
    "        print('{}: {}'.format(key,item))\n",
    "    return\n",
    "def get_down_quota():\n",
    "    return xmlrpc.ServerInfo(token)['download_limits']['client_download_quota']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Current Stats-----------------\n",
      "global_24h_download_limit: 200\n",
      "client_ip: 216.174.124.203\n",
      "limit_check_by: user_id\n",
      "user_id: 5937493\n",
      "client_24h_download_count: 1\n",
      "client_download_quota: 999\n",
      "client_24h_download_limit: 1000\n"
     ]
    }
   ],
   "source": [
    "server_stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path=os.getcwd()+'\\\\website\\\\flask_files\\\\model_db\\\\'\n",
    "subs_dir=os.getcwd()+'\\\\subs_storage\\\\'\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open(data_path + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=2)\n",
    "        \n",
    "def load_obj(name):\n",
    "    try:\n",
    "        with open(data_path + name + '.pkd', 'rb+') as fp:\n",
    "            return pickle.load(fp)\n",
    "    except:\n",
    "        with open(data_path + name + '.pkl', 'rb+') as fp:\n",
    "            return pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_database=[]\n",
    "for item in [item.split(\".\")[0] for item in os.listdir(data_path) if \"movie_DB\" in item  and \"temp\" not in item]:\n",
    "    movie_database+=load_obj(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdb_unique(movie_db):\n",
    "    return dict([(item['imdb_id'],item) for item in movie_database])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number movies in db:50366\n"
     ]
    }
   ],
   "source": [
    "mdbu=mdb_unique(movie_database)\n",
    "print('number movies in db:{}'.format(len(mdbu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Subtitle_ID's\n",
    "Using out database of IMDB_ID's we seek 3 Subtitle_ID's in order of quality. Best one is first. We will then attempt download of the best, and if that doesnt work, the next one and so on... We do the search in batches of 500 whichis the maximum allowed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_one(sr_one):\n",
    "    if not sr_one:\n",
    "        return ()\n",
    "    else:\n",
    "        try:\n",
    "            sr=[(item['SubDownloadsCnt'],item['SubRating'],item['IDSubtitleFile'],item['IDMovieImdb']) for item in sr_one \n",
    "             if item['SubFormat']=='srt' if not re.findall('.*[^a-zA-Z]cd[^a-zA-Z]',item['MovieReleaseName'].lower())]\n",
    "            sr_sort=sorted(sr, key=lambda x: -int(x[0]))\n",
    "            result=tuple([fill_id(sr_sort[0][3])]+[item[2] for item in sr_sort[0:3]])\n",
    "            #pdb.set_trace()\n",
    "            return result #returns a tuple \n",
    "        except:\n",
    "            return ()\n",
    "\n",
    "def fill_id(string):\n",
    "    #opensubtitles removes the 'tt' and all leading 0's . this function replaces them.\n",
    "    while len(string) < 7:\n",
    "        string = '0' + string\n",
    "    return 'tt' + string\n",
    "    \n",
    "def search_imdbid(list_imdbids):\n",
    "    #SearchSubtitles is crappy because it also gives you 500 search results\n",
    "    #useless to search for 500 movies at a time since each one might have ~100 results\n",
    "    #for safety i will search for 10 at a time ~ 2000 requests ~ 40per 10seconds ~ 10-15 mins maybe \n",
    "    search_params=[{'sublanguageid':'eng','imdbid':item.strip('tt')} for item in list_imdbids]\n",
    "    search_data=xmlrpc.SearchSubtitles(token,search_params)\n",
    "    sr=[]\n",
    "    try_num=0\n",
    "    while sr==[]:\n",
    "        if search_data['status'] == '200 OK':\n",
    "            sd=search_data['data'] #a long list of search results by imdb_ids\n",
    "            uniq_id=list(set([item['IDMovieImdb'] for item in sd]))\n",
    "            sr=[filter_one([item for item in sd if item['IDMovieImdb'] == idnum]) for idnum in uniq_id] \n",
    "            #filter_one takes a list of search results for a single imdbid and returns a tuple (imdbid, subid1,subid2,subid3)\n",
    "            #the above 3 lines will partition the search results sd into a list of lists where each inner list is for one imdbid\n",
    "            if sr==[]: #sr will be an empty list if no results are found (it is possible)\n",
    "                return []         \n",
    "        else:\n",
    "            if try_num==3: #gives up after 3 times\n",
    "                print('Tried 3 times. Giving up.')\n",
    "                return \n",
    "            try_num=try_num+1\n",
    "            print('Error Code: {} Retrying in 10 seconds.'.format(search_data['status']))    \n",
    "            time.sleep(10)\n",
    "    return sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take movie_database-queries opebsubs-returns sub_id list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████████████▍                        | 724/1000 [55:06<21:00,  4.57s/it]"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "sub_ids=[]\n",
    "LA=list(mdbu.keys())\n",
    "size=10\n",
    "for i in tqdm(range(0,10000,size)):\n",
    "    sub_ids=sub_ids+search_imdbid(LA[i:i+size])\n",
    "    if i%2000==0:\n",
    "        #print('\\nWriting data to file. {} movies so far. {:.3f} Percent Complete.'.format(i,(i/N_movies)*100))\n",
    "        save_obj(sub_ids, datetime.datetime.now().strftime('sub_id_%Y_%b_%d_final'))\n",
    "        \n",
    "sub_id_clean=[item for item in sub_ids if item!=()]\n",
    "save_obj(sub_id_clean, datetime.datetime.now().strftime('sub_id_%Y_%b_%d_final'))\n",
    "\n",
    "print('\\nElapsed {} seconds.'.format(time.time()-t0))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sub_ids 16893\n"
     ]
    }
   ],
   "source": [
    "sub_id_clean = load_obj('sub_id_clean')\n",
    "print('Number of sub_ids {}'.format(len(sub_id_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title(imdb_id):\n",
    "    try:\n",
    "        ind=[item['imdb_id'] for item in movie_database].index(imdb_id)\n",
    "        return movie_database[ind]['title'] \n",
    "    except:\n",
    "        return 'Not Found.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def down_sub(sub_id):\n",
    "    downsub=xmlrpc.DownloadSubtitles(token, [sub_id])\n",
    "    if downsub['status'] != '200 OK':\n",
    "        print('Error Code: {}'.format(downsub['status']))\n",
    "        server_stat()\n",
    "        sys.exit('Leaving this world now.')\n",
    "    return downsub\n",
    "\n",
    "def decode_sub(enc_subfile):\n",
    "    dec_data=decode(bytearray(enc_subfile['data'][0]['data'].encode(\"utf-8\")),'base64')\n",
    "    try:\n",
    "        sub_txt=gzip.GzipFile(fileobj=io.BytesIO(dec_data)).read().decode(\"utf-8\")       \n",
    "    except:\n",
    "        sub_txt=gzip.GzipFile(fileobj=io.BytesIO(dec_data)).read().decode(\"ISO-8859-1\")   \n",
    "    return sub_txt\n",
    "\n",
    "def write_to_file(file_name, file_content):\n",
    "    #writes file_content into an srt file in the subs_storage subfolder of current path\n",
    "    #if exists prints message\n",
    "    #if os.path.isfile(os.getcwd()+'\\\\subs_storage\\\\'+ file_name + '.srt'):\n",
    "    #    print('Sub '+ file_name + ' exists already.')\n",
    "    #else:\n",
    "    with open(os.getcwd()+'\\\\subs_storage\\\\'+ file_name +'.srt','wb') as fp:\n",
    "        fp.write(file_content.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS PART ACTIVELY DOWNLOADS\n",
    "Checks to see what *.srt files exist in the folder. \n",
    "1. If they are there nothing happens.\n",
    "2. If they are not there then it uses sub_id_clean file to get the best sub listing. \n",
    "\n",
    "<img src=\"http://clipartix.com/wp-content/uploads/2017/06/Stop-sign-clip-art-microsoft-free-clipart-images-2.png\" alt=\"Le bag of words\" title=\"The bag of words\" />\n",
    "\n",
    "**RUNNING THE NEXT CELL WILL CAUSE DOWNLOADING TO OCCUR. YOU ONLY HAVE 1000 PER DAY.**\n",
    "**MAKE SURE EVERYTHING IS GOOD BEFORE RUNINNG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in sub_id_clean:\n",
    "    if os.path.isfile(os.getcwd()+'\\\\subs_storage\\\\'+ item[0] + '.srt'):\n",
    "        continue\n",
    "        #print('SUB_ID:{:<10} IMDB_ID:{} exists ---- SKIPPING FILE Title: {}'.format(item[1],item[0],get_title(item[0])))\n",
    "    else:\n",
    "        print('SUB_ID:{:<10} IMDB_ID:{} downloading ------------- Title: {}'.format(item[1],item[0],get_title(item[0])))\n",
    "        write_to_file(item[0],decode_sub(down_sub(item[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Parsing SRT Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subs_dir=os.getcwd()+'\\\\subs_storage\\\\'\n",
    "def read_srt(imdb_id):\n",
    "    if os.path.isfile(subs_dir+imdb_id + '.srt'):\n",
    "        with open(subs_dir+imdb_id+'.srt','rb') as fp:\n",
    "            test=fp.read().decode('utf-8')\n",
    "        return test\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def fparse1(string):\n",
    "    if string:\n",
    "        #string=re.sub(r'<i>','',string)    \n",
    "        #string=re.sub(r'</i>','',string)\n",
    "        string=re.sub('<[^>]*.','',string) #kills anything inside < CACA >\n",
    "        string=re.sub(r'\\ufeff','',string)\n",
    "        #string=re.sub('^[^(1)]*','',string) #removes stuff before the 1\n",
    "        string=re.sub('^[^\\a]*?1\\r\\n00:','1\\r\\n00:',string) #removes stuff before the 1 betterly\n",
    "        return string\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def srt_conv(subs):\n",
    "    if subs:\n",
    "        starttimes=[]\n",
    "        endtimes=[]\n",
    "        words=[]\n",
    "        indexes=[]\n",
    "        #\"-->\" in line\n",
    "        subs=io.StringIO(subs)\n",
    "        prevLineWasWords =False\n",
    "        prevLineWasTime =False\n",
    "        for line in subs:\n",
    "            line=line.strip()\n",
    "            if \"-->\" in line:\n",
    "                starttimes.append(line[:8])\n",
    "                endtimes.append(line[17:25])\n",
    "                prevLineWasTime =True\n",
    "            elif line.strip().isdigit() and not prevLineWasTime:\n",
    "                indexes.append(int(line))\n",
    "                prevLineWasTime=False\n",
    "            else:\n",
    "            #elif len(line) > 1:\n",
    "                prevLineWasTime = False\n",
    "                if prevLineWasWords:\n",
    "                        words[len(words)-1] = words[len(words)-1] + \" \" + line\n",
    "                else:\n",
    "                        words.append(line)\n",
    "                prevLineWasWords = True\n",
    "                continue\n",
    "            prevLineWasWords = False  \n",
    "        \n",
    "        #trim because sometimes a file will end without a last tine.\n",
    "        trim_by=min([len(starttimes),len(endtimes),len(words)])\n",
    "        starttimes=starttimes[0:trim_by]\n",
    "        endtimes=endtimes[0:trim_by]\n",
    "        words=words[0:trim_by]\n",
    "        \n",
    "        return starttimes, endtimes, words, indexes\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def watch_output(clean_srt):\n",
    "    subs=io.StringIO(clean_srt)\n",
    "    print('=== is number, ~~~ is text, --- is time index. \\n')\n",
    "    prevLineWasTime=False\n",
    "    for line in subs:\n",
    "        if \"-->\" in line:\n",
    "            prevLineWasTime=True\n",
    "            print('----'+line)\n",
    "        elif line.strip().isdigit() and not prevLineWasTime:\n",
    "            prevLineWasTime=False\n",
    "            print('====='+line)\n",
    "        else:\n",
    "        #elif len(line)>1:\n",
    "            prevLineWasTime=False\n",
    "            print('~~~~'+line)\n",
    "    return\n",
    "\n",
    "def s_time(string):\n",
    "    try:\n",
    "        a=datetime.strptime(string, '%H:%M:%S')\n",
    "        return a\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def make_df(imdb_id):\n",
    "    srt1=fparse1(read_srt(imdb_id))\n",
    "    if srt1:\n",
    "        a,b,c,_=srt_conv(srt1)\n",
    "\n",
    "        imdb_idi=[imdb_id]\n",
    "        linfo=['StartTime','EndTime','Line']\n",
    "        index=pd.MultiIndex.from_product([imdb_idi,linfo],names=['IDs','LData'])\n",
    "        data_sub=pd.DataFrame(columns=index)\n",
    "\n",
    "\n",
    "        s1 = pd.Series([s_time(item) for item in a])\n",
    "        s2 = pd.Series([s_time(item) for item in b])\n",
    "        s3 = pd.Series(c)\n",
    "        data_sub.loc[:,(imdb_id,'StartTime')] =s1.values\n",
    "        data_sub.loc[:,(imdb_id,'EndTime')] = s2.values\n",
    "        data_sub.loc[:,(imdb_id,'Line')] = s3.values\n",
    "\n",
    "        return data_sub\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def make_tup(imdb_id):\n",
    "    srt1=fparse1(read_srt(imdb_id))\n",
    "    if srt1:\n",
    "        a,b,c,_=srt_conv(srt1)\n",
    "\n",
    "        s1 = [s_time(item) for item in a]\n",
    "        s2 = [s_time(item) for item in b]\n",
    "        s3 = c\n",
    "\n",
    "        return (s1,s2,s3)\n",
    "    else:\n",
    "        pass    \n",
    "\n",
    "def append_pd(df1,df):\n",
    "    tes=pd.concat([df1,df], axis=1)\n",
    "    tes=tes.fillna('')\n",
    "    return tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aid=[item.split('.')[0] for item in listdir(subs_dir) if 'tt' in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. 0 - adding tt0002381 - elapsed 0.058014869689941406 sec.\n",
      "No. 100 - adding tt0029583 - elapsed 2.318572759628296 sec.\n",
      "No. 200 - adding tt0037522 - elapsed 6.224541425704956 sec.\n",
      "No. 300 - adding tt0044081 - elapsed 10.13450312614441 sec.\n",
      "No. 400 - adding tt0049949 - elapsed 13.515328407287598 sec.\n",
      "No. 500 - adding tt0056193 - elapsed 17.217242002487183 sec.\n",
      "No. 600 - adding tt0062190 - elapsed 20.68910241127014 sec.\n",
      "No. 700 - adding tt0067328 - elapsed 24.059925317764282 sec.\n",
      "No. 800 - adding tt0072431 - elapsed 27.283752918243408 sec.\n",
      "No. 900 - adding tt0078736 - elapsed 31.405113220214844 sec.\n",
      "No. 1000 - adding tt0082817 - elapsed 35.27142262458801 sec.\n",
      "No. 1100 - adding tt0086837 - elapsed 39.01010704040527 sec.\n",
      "No. 1200 - adding tt0090297 - elapsed 42.65796709060669 sec.\n",
      "No. 1300 - adding tt0093223 - elapsed 46.41611313819885 sec.\n",
      "No. 1400 - adding tt0096328 - elapsed 50.37458324432373 sec.\n",
      "No. 1500 - adding tt0099557 - elapsed 54.393736600875854 sec.\n",
      "No. 1600 - adding tt0103010 - elapsed 58.84969234466553 sec.\n",
      "No. 1700 - adding tt0106664 - elapsed 63.26111578941345 sec.\n",
      "No. 1800 - adding tt0110148 - elapsed 67.71125650405884 sec.\n",
      "No. 1900 - adding tt0113748 - elapsed 72.28705501556396 sec.\n",
      "No. 2000 - adding tt0117420 - elapsed 76.65920853614807 sec.\n",
      "No. 2100 - adding tt0119558 - elapsed 81.30054354667664 sec.\n",
      "No. 2200 - adding tt0120783 - elapsed 85.82652688026428 sec.\n",
      "No. 2300 - adding tt0141109 - elapsed 90.08590173721313 sec.\n",
      "No. 2400 - adding tt0178821 - elapsed 94.37798476219177 sec.\n",
      "No. 2500 - adding tt0217086 - elapsed 98.87784337997437 sec.\n",
      "No. 2600 - adding tt0255798 - elapsed 103.56446647644043 sec.\n",
      "No. 2700 - adding tt0283897 - elapsed 107.87616205215454 sec.\n",
      "No. 2800 - adding tt0313016 - elapsed 112.17071390151978 sec.\n",
      "No. 2900 - adding tt0335563 - elapsed 116.47238564491272 sec.\n",
      "No. 3000 - adding tt0363988 - elapsed 120.79032874107361 sec.\n",
      "No. 3100 - adding tt0378109 - elapsed 124.91600584983826 sec.\n",
      "No. 3200 - adding tt0401504 - elapsed 129.05682945251465 sec.\n",
      "No. 3300 - adding tt0421729 - elapsed 133.30465030670166 sec.\n",
      "No. 3400 - adding tt0443465 - elapsed 137.77633833885193 sec.\n",
      "No. 3500 - adding tt0458465 - elapsed 142.05896544456482 sec.\n",
      "No. 3600 - adding tt0478970 - elapsed 146.29290199279785 sec.\n",
      "No. 3700 - adding tt0765442 - elapsed 150.4309060573578 sec.\n",
      "No. 3800 - adding tt0816442 - elapsed 154.67942929267883 sec.\n",
      "No. 3900 - adding tt0890870 - elapsed 158.97702932357788 sec.\n",
      "No. 4000 - adding tt0980970 - elapsed 162.92272758483887 sec.\n",
      "No. 4100 - adding tt1055366 - elapsed 167.1441249847412 sec.\n",
      "No. 4200 - adding tt1124039 - elapsed 171.3637707233429 sec.\n",
      "No. 4300 - adding tt1190910 - elapsed 175.42631077766418 sec.\n",
      "No. 4400 - adding tt1240982 - elapsed 179.83794808387756 sec.\n",
      "No. 4500 - adding tt1294138 - elapsed 183.71602535247803 sec.\n",
      "No. 4600 - adding tt1351685 - elapsed 187.9449381828308 sec.\n",
      "No. 4700 - adding tt1438030 - elapsed 192.60759043693542 sec.\n",
      "No. 4800 - adding tt1521979 - elapsed 196.8272843360901 sec.\n",
      "No. 4900 - adding tt1582507 - elapsed 200.9011526107788 sec.\n",
      "No. 5000 - adding tt1655442 - elapsed 205.05972480773926 sec.\n",
      "No. 5100 - adding tt1728638 - elapsed 209.7948875427246 sec.\n",
      "No. 5200 - adding tt1828970 - elapsed 214.43752026557922 sec.\n",
      "No. 5300 - adding tt1935302 - elapsed 218.9918909072876 sec.\n",
      "No. 5400 - adding tt2044801 - elapsed 223.3903293609619 sec.\n",
      "No. 5500 - adding tt2140479 - elapsed 227.79424405097961 sec.\n",
      "No. 5600 - adding tt2275946 - elapsed 232.54801988601685 sec.\n",
      "No. 5700 - adding tt2369135 - elapsed 237.11318969726562 sec.\n",
      "No. 5800 - adding tt2488496 - elapsed 241.63574719429016 sec.\n",
      "No. 5900 - adding tt2674454 - elapsed 246.0110318660736 sec.\n",
      "No. 6000 - adding tt2937366 - elapsed 250.72961378097534 sec.\n",
      "No. 6100 - adding tt3121860 - elapsed 255.37599110603333 sec.\n",
      "No. 6200 - adding tt3322940 - elapsed 259.742164850235 sec.\n",
      "No. 6300 - adding tt3553442 - elapsed 264.2623291015625 sec.\n",
      "No. 6400 - adding tt3774466 - elapsed 268.81270360946655 sec.\n",
      "No. 6500 - adding tt4062536 - elapsed 273.30089926719666 sec.\n",
      "No. 6600 - adding tt4425200 - elapsed 277.59306478500366 sec.\n",
      "No. 6700 - adding tt4827878 - elapsed 282.5008339881897 sec.\n",
      "No. 6800 - adding tt5352846 - elapsed 286.83997440338135 sec.\n",
      "No. 6900 - adding tt5896146 - elapsed 291.5957806110382 sec.\n",
      "elapsed time 296.3917763233185 seconds\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "full_data={}\n",
    "for i in range(len(aid)):\n",
    "    full_data[aid[i]]=make_tup(aid[i])\n",
    "    if i%100==0:\n",
    "        print('No. {} - adding {} - elapsed {} sec.'.format(i,aid[i],time.time()-t))\n",
    "t1=time.time()\n",
    "print('elapsed time {} seconds'.format(t1-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
